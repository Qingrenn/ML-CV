{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loadmat('ex4data1.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1数据初始化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['X']\n",
    "Y = data['y']\n",
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False) #不采用稀疏表示法\n",
    "Y_onehot = encoder.fit_transform(Y)\n",
    "Y_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该数据集，一共有5000个样本，每个样本400个特征（还要再增加1个偏置）。\n",
    "\n",
    "所以需要构建的三层神经网络的输入层为400+1个输入单元，隐藏层为25+1个单元，输出层为10个单元（对应着Y的one-hot编码标签）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2前向传播与代价函数\n",
    "sigmoid函数：\n",
    "<center>\n",
    "    $g(z)=\\frac{1}{1+e^{-z}}$\n",
    "</center>\n",
    "假设函数：\n",
    "<center>\n",
    "    $h_{\\theta}(X)=$ $\\frac{1}{1+e^{-\\theta^{T}X}}$\n",
    "</center>\n",
    "代价函数：\n",
    "<center>\n",
    "    $J(\\theta)=-\\frac{1}{M}\\sum\\limits_{i=1}^{M}\\sum\\limits_{k=1}^{K}(1-Y^{(i)}_k)\\ln[1-h_{\\theta}(X^{(i)})_k]+Y^{(i)}_k\\ln[h_\\theta(X^{(i)})_k]  \\\\ + \\frac{\\lambda}{2M}[\\sum\\limits_{j=1}^{25}\\sum\\limits_{k=1}^{400} (\\Theta_{jk}^{(1)})^2+\\sum\\limits_{j=1}^{10}\\sum\\limits_{k=1}^{25}(\\Theta_{jk}^{(2)})^2]$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X:(5000,400),theta1(25,401),theta2(10,26)\n",
    "def forward_propogate(X,theta1,theta2):\n",
    "    a1 = np.insert(X,0,1,axis=1)\n",
    "    z2 = a1@theta1.T\n",
    "    a2 = np.insert(sigmoid(z2),0,1,axis=1)\n",
    "    z3 = a2@theta2.T\n",
    "    h = sigmoid(z3)\n",
    "    return a1,z2,a2,z3,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta1:(hidden_size,input_size+1) theta2(num_labels,hidden_size+1)\n",
    "#parameters( hidden_size*(input_size+1) + num_labels*(hidden_size+1) , )\n",
    "def computeCost(parameters,input_size,hidden_size,num_labels,X,Y_onehot,learnigRate):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    theta1 = np.reshape( parameters[:hidden_size*(input_size+1)], ( hidden_size, input_size + 1 ) )\n",
    "    theta2 = np.reshape( parameters[hidden_size*(input_size+1):] , ( num_labels, hidden_size + 1 ) )\n",
    "    \n",
    "    a1,z2,a2,z3,h = forward_propogate(X,theta1,theta2)\n",
    "    \n",
    "    J1 = 0\n",
    "    for i in range(m):\n",
    "        J1 += np.sum( (1-Y_onehot[i,:]) * np.log(1-h[i,:]) + Y_onehot[i,:] * np.log(h[i,:]) )\n",
    "    J1 = -J1/m\n",
    "    J2 = learnigRate/(2*m) * ( np.sum( np.power(theta1[:,1:],2) ) + np.sum( np.power(theta2[:,1:],2) ))\n",
    "    \n",
    "    return J1+J2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401) (10, 26)\n",
      "(5000, 401) (5000, 25) (5000, 26) (5000, 10) (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "input_size = 400\n",
    "hidden_size = 25\n",
    "num_labels = 10\n",
    "\n",
    "#np.random.seed()\n",
    "parameters = ( np.random.random( size = hidden_size*(input_size+1)+num_labels*(hidden_size+1) ) - 0.5 )*0.25\n",
    "\n",
    "m = X.shape[0]\n",
    "\n",
    "theta1 = np.reshape( parameters[:hidden_size*(input_size+1)], ( hidden_size, input_size + 1 ) )\n",
    "theta2 = np.reshape( parameters[hidden_size*(input_size+1):] , ( num_labels, hidden_size + 1 ) )\n",
    "print(theta1.shape,theta2.shape)\n",
    "\n",
    "a1,z2,a2,z3,h = forward_propogate(X,theta1,theta2)\n",
    "print(a1.shape,z2.shape,a2.shape,z3.shape,h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.678943926992122"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(parameters,input_size,hidden_size,num_labels,X,Y_onehot,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3反向传播算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第t个样本的代价函数$J^{(t)}$对第t个样本所对应的$z_3$中的第k个元素$z_{3k}^{(t)}$求导：\n",
    "<center>\n",
    "$\n",
    "\\delta_{3k}^{(t)} = \\frac{\\partial}{\\partial z_{3k}^{(t)}}J^{(t)} = h^{(t)}_k - y^{(t)}_k \\Rightarrow \\delta_3^{(t)} =  \\frac{\\partial}{\\partial z_{3}^{(t)}}J^{(t)}= h^{(t)}-y^{(t)}\n",
    "$\n",
    "</center>\n",
    "\n",
    "可以推出第t个样本的代价函数对其所对应的$z_3$求导为一个**行向量**\n",
    "\n",
    "同理，第t个样本的代价函数$J^{(t)}$对第t个样本所对应的$z_2$中的第k个元素$z_{2k}^{(t)}$求导：\n",
    "\n",
    "<center>\n",
    "$\n",
    "\\delta_{2k}^{(t)} = \\frac{\\partial}{\\partial z_{2k}^{(t)}}J^{(t)} = \\frac{\\partial J^{(t)}}{\\partial a_{2k}^{(t)}} * \\frac{\\partial a_{2k}^{(t)}}{\\partial z_{2k}^{(t)}} \\Rightarrow \\delta_{2}^{(t)} = \\frac{\\partial}{\\partial z_{2}^{(t)}}J^{(t)} = \\frac{\\partial J^{(t)}}{\\partial a_{2}^{(t)}} * \\frac{\\partial a_{2}^{(t)}}{\\partial z_{2}^{(t)}}\n",
    "$\n",
    "</center>\n",
    "\n",
    "可以推出第t个样本的代价函数对其所对应的$z_2$求导为两个**行向量**的哈达玛积\n",
    "\n",
    "又有：\n",
    "<center>\n",
    "$\n",
    "z_3^{(t)} = a_2^{(t)}@(\\Theta^{(2)})^T \\\\\n",
    "\\Rightarrow \\frac{\\partial J^{(t)}}{\\partial a^{(t)}_2} = \\frac{\\partial J^{(t)}}{\\partial z_3^{(t)}}@\\Theta^{(2)}\n",
    "$\n",
    "</center>\n",
    "\n",
    "将$\\frac{\\partial J^{(t)}}{\\partial a^{(t)}_2} =\\delta_3^{(t)}@\\Theta^{(2)}$和$\\frac{\\partial a_{2}^{(t)}}{\\partial z_{2}^{(t)}} = a_2^{(t)}*(1 -a_2^{(t)})$带入$\\delta_2^{(t)}$的表达式中可得：\n",
    "\n",
    "<center>\n",
    "$\n",
    "\\delta_2^{(t)} =\\delta_3^{(t)}@\\Theta^{(2)}*a_2^{(t)}*(1 -a_2^{(t)})\n",
    "$\n",
    "</center>\n",
    "\n",
    "各矩阵维度为：\n",
    "<center>\n",
    "$\n",
    "\\text{(1,num_labels)@(num_labels,hidden_size+1) * (1,hidden_size+1)* (1,hidden_size+1)}\n",
    "$\n",
    "</center>\n",
    "\n",
    "最终$\\delta_2^{(t)}$的维度$\\text{(1,hidden_size+1)}$，但是当在下面计算$\\frac{\\partial}{\\partial \\theta_1}J^{(t)}$的时候要去掉一维,因为$z_2$并没有扩展一维，是$a_2$才扩展出一维偏置的\n",
    "\n",
    "而$\\frac{\\partial}{\\partial \\theta_{2}^{(t)}}J^{(t)}$与$\\frac{\\partial}{\\partial z_{3}^{(t)}}J^{(t)} \\quad (z_3^{(t)} = a_2^{(t)}@(\\Theta^{(2)})^T )$可以建立联系、$\\frac{\\partial}{\\partial \\theta_{1}^{(t)}}J^{(t)}$与$\\frac{\\partial}{\\partial z_{2}^{(t)}}J^{(t)} \\quad (z_2^{(t)} = a_1^{(t)}@(\\Theta^{(1)})^T )$也可以建立联系\n",
    "\n",
    "联系如下：\n",
    "<center>\n",
    "$\n",
    "\\frac{\\partial}{\\partial \\theta_2}J^{(t)} = (\\delta_3^{(t)})^{T}a_2^{(t)} \\quad \\text{(1,num_label)}^T@\\text{(1,hidden_size+1)} = \\text{(num_label,hidden_size+1)}\n",
    "\\\\\n",
    "\\frac{\\partial}{\\partial \\theta_1}J^{(t)} = (\\delta_2^{(t)})^{T}a_1^{(t)} \\quad \\text{(1,hidden_size)}^T@\\text{(1,input_size+1)} = \\text{(hidden_size,input_size+1)}\n",
    "$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propogate(parameters,input_size,hidden_size,num_labels,X,Y_onehot,learnig_rate):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    theta1 = np.reshape( parameters[:hidden_size*(input_size+1)], ( hidden_size, input_size + 1 ) )\n",
    "    theta2 = np.reshape( parameters[hidden_size*(input_size+1):] , ( num_labels, hidden_size + 1 ) )\n",
    "    \n",
    "    #前向传播\n",
    "    a1,z2,a2,z3,h = forward_propogate(X,theta1,theta2)\n",
    "    \n",
    "    #计算代价\n",
    "    J1 = 0\n",
    "    for i in range(m):\n",
    "        J1 += np.sum( (1-Y_onehot[i,:]) * np.log(1-h[i,:]) + Y_onehot[i,:] * np.log(h[i,:]) )\n",
    "    \n",
    "    J1 = -J1/m\n",
    "    J2 = float(learnig_rate)/(2*m) * ( np.sum( np.power(theta1[:,1:],2) ) + np.sum( np.power(theta2[:,1:],2) ))\n",
    "    J = J1+J2\n",
    "    \n",
    "    #反向传播\n",
    "    theta1_gradient = np.zeros(theta1.shape)\n",
    "    theta2_gradient = np.zeros(theta2.shape)\n",
    "    \n",
    "    for t in range(m):\n",
    "        a1_t = np.reshape(a1[t,:],(1,a1.shape[1])) #(1,401)\n",
    "        z2_t = np.reshape(z2[t,:],(1,z2.shape[1]))  #(1,25)\n",
    "        a2_t = np.reshape(a2[t,:],(1,a2.shape[1])) #(1,26)\n",
    "        z3_t = np.reshape(z3[t,:],(1,z3.shape[1])) #(1,10)\n",
    "        h_t = np.reshape(h[t,:],(1,h.shape[1]))   #(1,10)\n",
    "        y_t = np.reshape(Y_onehot[t,:],(1,Y_onehot.shape[1])) #(1,10)\n",
    "        \n",
    "        delta3_t = h_t -y_t #(1,10)\n",
    "        delta2_t = (delta3_t@theta2)*a2_t*(1-a2_t) #(1,26)\n",
    "                \n",
    "        theta1_gradient = theta1_gradient + (delta2_t[:,1:]).T * a1_t #(1,25)^T * (1,401) = (25,401)  \n",
    "        theta2_gradient = theta2_gradient + delta3_t.T * a2_t #(1,10)^T * (1,26)\n",
    "    \n",
    "    theta1_gradient = theta1_gradient/m\n",
    "    theta2_gradient = theta2_gradient/m\n",
    "    \n",
    "    #添加正则项的梯度\n",
    "    theta1_gradient[:,1:] = theta1_gradient[:,1:] + (theta1[:,1:] * learnig_rate)/m \n",
    "    theta2_gradient[:,1:] = theta2_gradient[:,1:] + (theta2[:,1:] * learnig_rate)/m\n",
    "    \n",
    "    grad = np.concatenate( ( np.ravel(theta1_gradient),np.ravel(theta2_gradient) ) ,axis=0)\n",
    "    \n",
    "    return J,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.678943926992122, (10285,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J,grad = back_propogate(parameters,input_size,hidden_size,num_labels,X,Y_onehot,1)\n",
    "J,grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4训练神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.32652905464389775\n",
       "     jac: array([-3.58440320e-05, -9.80057597e-08, -7.18193476e-07, ...,\n",
       "       -7.04974398e-06,  1.43780461e-05, -3.91182132e-05])\n",
       " message: 'Max. number of function evaluations reached'\n",
       "    nfev: 250\n",
       "     nit: 22\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([ 5.59450436e-01, -4.90028799e-04, -3.59096738e-03, ...,\n",
       "       -7.72181778e-01,  1.11884859e+00,  6.22469685e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "res = minimize(fun=back_propogate,x0=parameters,args=(input_size,hidden_size,num_labels,X,Y_onehot,1),method='TNC',jac=True,options={'maxiter':250})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10, 10, ...,  9,  9,  9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1_res = np.reshape(res.x[:hidden_size * (input_size + 1)],(hidden_size,input_size + 1))\n",
    "theta2_res = np.reshape(res.x[hidden_size * (input_size + 1):],(num_labels,hidden_size+1))\n",
    "\n",
    "a1_res,z2_res,a2_res,z3_res,h_res = forward_propogate(X,theta1_res,theta2_res)\n",
    "y_pred = np.array(np.argmax(h_res,axis=1)+1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 99.42%\n"
     ]
    }
   ],
   "source": [
    "correct = [1 if a == b else 0 for (a,b) in zip(y_pred,Y)]\n",
    "accuracy = sum(map(int,correct)) / len(correct)\n",
    "print('accuracy = {0}%'.format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
